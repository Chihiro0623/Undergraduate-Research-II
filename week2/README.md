# 연구노트 2주차 (7/10 - 7/14)
## 활동 내용
Knowledge distillation 논문 Review  
Classification 모델 구현

## 논문 Review
| Week   | Paper                                               | Conf | Year   | Review   |
| :----: | ------------------------------------------------------- | :----: | :------------: | :------: |
| 2    | [Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)<br>[CBAM: Convolutional Block Attention Module](https://arxiv.org/pdf/1807.06521.pdf)<br>[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/pdf/1905.11946.pdf)<br>[Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf) | CVPR<br>ECCV<br>ICML<br>NIPs    |2018<br>2018<br>2019<br>2014 | [Review](https://github.com/Chihiro0623/2023summer-selfstudy1/blob/main/week2/Reviews/Squeeze-and-Excitation%20Networks.pdf)<br>[Review](https://github.com/Chihiro0623/2023summer-selfstudy1/blob/main/week2/Reviews/CBAM%20Convolutional%20Block%20Attention%20Module.pdf)<br>[Review]()<br>[Review]() |



### Squeeze-and-Excitation Networks
Channel-wise Training의 중요성을 알려준 논문이다.

### CBAM: Convolutional Block Attention Module
Attention 기법을 이용하여 Channel-wise뿐만 아니라 Spatial도 고려한 모듈을 고안한 논문이다.

### EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Depth Width Resolution 세 방향을 같은 비율으로 키우는 것이 중요하다는 것을 밝혀낸 논문이다.

### Distilling the Knowledge in a Neural Network
전문가 모델들을 통한 Ensemble과 Soft Label을 소개한 논문이다.



## 멘토멘티 프로젝트
[Assignment 1](https://github.com/Chihiro0623/2023summer-selfstudy1/blob/main/week1/Project/week1.pdf)  
[WandB](https://wandb.ai/oso0310/project1/reports/Assignment1--Vmlldzo1MDIwMTU2?accessToken=bq67s6k5ze7547kxh6f6u23bu1fi5h11pvzzbea80w39ekar54i3f7ydcpq9ofau)  
[보고서](https://github.com/Chihiro0623/2023summer-selfstudy1/blob/main/week2/Project/Assignment1.pdf)  
[코드](https://github.com/Chihiro0623/2023summer-selfstudy1/tree/main/week2/Project/Assignment1)  
